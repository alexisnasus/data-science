
"""
Script 3: Scraper Incremental
Scrapea datos periÃ³dicamente y los agrega a un archivo CSV histÃ³rico

Uso:
    python 03_scraper_incremental.py
    
Output: propiedades_historico.csv (se va actualizando)
"""

import pandas as pd
from datetime import datetime
import os
from pathlib import Path
import sys
import importlib.util

# Importar la clase del script 01
spec = importlib.util.spec_from_file_location("scraper", "01_scraper_simple.py")
scraper_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(scraper_module)
PortalInmobiliarioScraper = scraper_module.PortalInmobiliarioScraper


def cargar_datos_existentes(archivo: str) -> pd.DataFrame:
    """Carga datos existentes o retorna DataFrame vacÃ­o"""
    if os.path.exists(archivo):
        print(f"ğŸ“‚ Cargando datos existentes: {archivo}")
        df = pd.read_csv(archivo, encoding='utf-8-sig')
        print(f"   Registros anteriores: {len(df)}")
        return df
    else:
        print("ğŸ“ Archivo no existe, se crearÃ¡ uno nuevo")
        return pd.DataFrame()


def limpiar_duplicados(df: pd.DataFrame) -> pd.DataFrame:
    """Elimina propiedades duplicadas por URL"""
    antes = len(df)
    df = df.drop_duplicates(subset=['url'], keep='last')
    despues = len(df)
    eliminados = antes - despues
    
    if eliminados > 0:
        print(f"ğŸ§¹ Eliminados {eliminados} duplicados")
    
    return df


def main():
    """FunciÃ³n principal"""
    print("="*70)
    print("  ğŸ  SCRAPER INCREMENTAL - ACTUALIZACIÃ“N DE DATOS HISTÃ“RICOS")
    print("="*70)
    
    # ConfiguraciÃ³n
    URL_BASE = "https://www.portalinmobiliario.com/arriendo/casa/santiago-metropolitana"
    NUM_PAGINAS = 5
    DELAY = 3
    ARCHIVO_HISTORICO = 'propiedades_historico.csv'
    
    print(f"\nğŸ“‹ CONFIGURACIÃ“N:")
    print(f"  â€¢ URL: {URL_BASE}")
    print(f"  â€¢ PÃ¡ginas por actualizaciÃ³n: {NUM_PAGINAS}")
    print(f"  â€¢ Archivo histÃ³rico: {ARCHIVO_HISTORICO}")
    
    # Cargar datos existentes
    df_existente = cargar_datos_existentes(ARCHIVO_HISTORICO)
    
    # Scrapear nuevos datos
    print("\nğŸš€ Iniciando scraping...")
    scraper = PortalInmobiliarioScraper()
    propiedades_nuevas = scraper.scrape_multiples_paginas(URL_BASE, NUM_PAGINAS, DELAY)
    
    if not propiedades_nuevas:
        print("âŒ No se extrajeron propiedades nuevas")
        return
    
    # Crear DataFrame de nuevos datos
    df_nuevos = pd.DataFrame(propiedades_nuevas)
    
    # Agregar timestamp de scraping
    df_nuevos['fecha_scraping'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    print(f"\nğŸ“Š Nuevos datos:")
    print(f"   Propiedades extraÃ­das: {len(df_nuevos)}")
    
    # Combinar con datos existentes
    if not df_existente.empty:
        df_combinado = pd.concat([df_existente, df_nuevos], ignore_index=True)
    else:
        df_combinado = df_nuevos
    
    # Limpiar duplicados
    df_final = limpiar_duplicados(df_combinado)
    
    # Guardar
    df_final.to_csv(ARCHIVO_HISTORICO, index=False, encoding='utf-8-sig')
    
    # Crear tambiÃ©n backup con timestamp
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    archivo_backup = f'backup_propiedades_{timestamp}.csv'
    df_final.to_csv(archivo_backup, index=False, encoding='utf-8-sig')
    
    # Resumen
    print("\n" + "="*70)
    print("  ğŸ“Š RESUMEN")
    print("="*70)
    print(f"  ğŸ“‚ Registros anteriores: {len(df_existente)}")
    print(f"  â• Nuevos registros: {len(df_nuevos)}")
    print(f"  ğŸ“Š Total en histÃ³rico: {len(df_final)}")
    print(f"  ğŸ’¾ Archivo principal: {ARCHIVO_HISTORICO}")
    print(f"  ğŸ’¾ Backup: {archivo_backup}")
    print("="*70)
    
    # EstadÃ­sticas
    print(f"\nğŸ“ˆ ESTADÃSTICAS DEL HISTÃ“RICO:")
    print(f"  â€¢ Precio promedio: ${df_final['precio'].mean():,.0f}")
    print(f"  â€¢ Precio mÃ­nimo: ${df_final['precio'].min():,.0f}")
    print(f"  â€¢ Precio mÃ¡ximo: ${df_final['precio'].max():,.0f}")
    print(f"  â€¢ Comunas Ãºnicas: {df_final['comuna'].nunique()}")
    print(f"  â€¢ Propiedades Ãºnicas: {df_final['url'].nunique()}")
    
    if 'fecha_scraping' in df_final.columns:
        fechas_unicas = df_final['fecha_scraping'].nunique()
        print(f"  â€¢ Actualizaciones realizadas: {fechas_unicas}")
    
    print(f"\nâœ… Â¡ActualizaciÃ³n completada!\n")


if __name__ == "__main__":
    main()
